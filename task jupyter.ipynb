{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie rekrutacyjne\n",
    "##### Plik należy pobrać z serwera, korzystam więc z komendy w terminalu bash 'sftp data-engineer@hiring.cloudtechnologies.dev' a następnie podaje hasło. Kolejnym krokiem jest pobranie pliku komendą 'get data.tsv.gz'. Na koniec zamykam połączenie komendą 'exit'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Na początek importuje potrzebne biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozpakowuje plik do formatu tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data.tsv.gz'\n",
    "output_file = 'unziped_data.tsv'\n",
    "\n",
    "# Rozpakowanie pliku\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f\"Plik rozpakowany do {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzam maksymalną ilość kolumn w pliku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiuje funkcję, sprawdzającą ilość kolumn\n",
    "def get_max_columns(output_file):\n",
    "    max_columns = 0\n",
    "    with open(output_file, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            num_columns = len(line)\n",
    "            if num_columns > max_columns:\n",
    "                max_columns = num_columns\n",
    "    return max_columns\n",
    "\n",
    "# Zapisuje do zmiennej wartość z liczbą kolumn\n",
    "max_columns = get_max_columns(output_file)\n",
    "\n",
    "print(f\"Maksymalna liczba kolumn w pliku: {max_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Następnym krokiem będzie oczyszczenie danych\n",
    "##### Nie mogę bezpośrednio wczytać danych do DataFrame w Pandas, ponieważ dane mają różne wymiary kolumn i typy. Dlatego wybieram opcję oczyszczenia pętlą standardowym Pythonem na początek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ścieżki plików\n",
    "input_file = 'unziped_data.tsv'\n",
    "output_file = 'final_cleaned_data.tsv'\n",
    "\n",
    "# Definiuję funkcję czyszczącą plik wraz z funkcjami warunkowymi\n",
    "def is_valid_number(value, min_length=14, max_length=22):\n",
    "    \"\"\"Sprawdza, czy wartość jest liczbą o długości od min_length do max_length cyfr.\"\"\"\n",
    "    return re.match(f'^\\d{{{min_length},{max_length}}}$', value) is not None\n",
    "\n",
    "def is_valid_unix_timestamp(value):\n",
    "    \"\"\"Sprawdza, czy wartość jest poprawnym UNIX timestamp (13 cyfr).\"\"\"\n",
    "    return re.match(r'^\\d{13}$', value) is not None\n",
    "\n",
    "def is_valid_url(value):\n",
    "    \"\"\"Sprawdza, czy wartość jest poprawnym adresem URL zaczynającym się od http:// lub https://.\"\"\"\n",
    "    return re.match(r'^https?://', value) is not None\n",
    "\n",
    "def is_valid_slash(value):\n",
    "    \"\"\"Sprawdza, czy wartość zawiera znak / jako stały element podstron\"\"\"\n",
    "    return '/' in value\n",
    "\n",
    "def ensure_full_url(value):\n",
    "    \"\"\"Dodaje https://www. do adresu, jeśli nie zaczyna się od http:// lub https://\"\"\"\n",
    "    if not is_valid_url(value):\n",
    "        return f'https://www.{value}'\n",
    "    return value\n",
    "\n",
    "def clean_data(input_file, output_file):\n",
    "    invalid_rows_count = 0\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile, delimiter='\\t')\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "\n",
    "        for line in reader:\n",
    "            # Maksymalna ilosć kolumn to 4, więc najpierw dla nich wykonuję operacje\n",
    "            if len(line) == 4:\n",
    "                col1, col2, col3, col4 = line\n",
    "\n",
    "                # Krok 1: Sprawdzanie poprawności ID i poprawne umiejscowienie\n",
    "                if not is_valid_number(col1):\n",
    "                    if is_valid_number(col2):\n",
    "                        col1, col2 = col2, col1\n",
    "                    elif is_valid_number(col3):\n",
    "                        col1, col3 = col3, col1\n",
    "                    elif is_valid_number(col4):\n",
    "                        col1, col4 = col4, col1\n",
    "                    else:\n",
    "                        continue    \n",
    "                \n",
    "                # 2: Sprawdzanie i poprawne umiejscowienie Unix Timestamp\n",
    "                if not is_valid_unix_timestamp(col3):\n",
    "                    if is_valid_unix_timestamp(col2):\n",
    "                        col3, col2 = col2, col3\n",
    "                    elif is_valid_unix_timestamp(col4):\n",
    "                        col3, col4 = col4, col3\n",
    "                    else:\n",
    "                        continue    \n",
    "                \n",
    "                # 3: Sprawdzanie i poprawianie adresu URL\n",
    "                if not is_valid_url(col2):\n",
    "                    if is_valid_url(col4):\n",
    "                        col2, col4 = col4, col2\n",
    "                        if is_valid_url(col2):\n",
    "                            if is_valid_slash(col4):\n",
    "                                col2 = f'{col2}{col4}'\n",
    "                        else:\n",
    "                            col2 = ensure_full_url(col2)\n",
    "                            if is_valid_slash(col4):\n",
    "                                col2 = f'{col2}{col4}'\n",
    "                if is_valid_url(col2):\n",
    "                    if is_valid_slash(col4):\n",
    "                            col2 = f'{col2}{col4}'   \n",
    "\n",
    "                # 4: Czyszczenie kolumny 4 i zapis rekordów\n",
    "                col4 = ''\n",
    "                writer.writerow([col1, col2, col3])\n",
    "                continue\n",
    "\n",
    "            # Docelowa ilość kolumn to 3, gdy rekord ma mniejszą ilość to dane są wybrakowane, więc zakładam pozbycie się ich  \n",
    "            elif len(line) == 3:\n",
    "                col1, col2, col3 = line\n",
    "                # 1: Czyszczenie kolumny 4 (profilaktycznie)\n",
    "                col4 = ''\n",
    "                \n",
    "                # 2: Sprawdzanie poprawności ID i poprawne umiejscowienie\n",
    "                if not is_valid_number(col1):\n",
    "                    if is_valid_number(col2):\n",
    "                        col1, col2 = col2, col1\n",
    "                    elif is_valid_number(col3):\n",
    "                        col1, col3 = col3, col1\n",
    "                    else:\n",
    "                        continue    \n",
    "                \n",
    "                # 3: Sprawdzanie i poprawne umiejscowienie Unix Timestamp\n",
    "                if not is_valid_unix_timestamp(col3):\n",
    "                    if is_valid_unix_timestamp(col2):\n",
    "                        col3, col2 = col2, col3\n",
    "                    else:\n",
    "                        continue    \n",
    "                \n",
    "                # 4: Sprawdzanie i poprawianie adresu URL\n",
    "                if not is_valid_url(col2):\n",
    "                    if is_valid_url(col3):\n",
    "                        col2, col3 = col3, col2\n",
    "                    else:\n",
    "                        col2 = ensure_full_url(col2)\n",
    "                        if is_valid_slash(col3):\n",
    "                            col2 = f'{col2}{col3}'\n",
    "                        elif is_valid_slash(col1):\n",
    "                            col2 = f'{col2}{col1}'   \n",
    "\n",
    "                # 5 Zapisanie poprawionych danych w kolumnach\n",
    "                writer.writerow([col1, col2, col3])\n",
    "                continue  \n",
    "\n",
    "            else:\n",
    "                invalid_rows_count += 1\n",
    "                continue  \n",
    "\n",
    "    print(f\"Dane zostały oczyszczone i zapisane do {output_file}\")\n",
    "    print(f\"Liczba wierszy z mniej niż 3 kolumnami: {invalid_rows_count}\")\n",
    "\n",
    "# Wywołanie funkcji\n",
    "clean_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja danych po 1 etapie czyszczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiuje funkcję liczącą ilość wierszy i porównującą między plikami\n",
    "\n",
    "def count_rows(file_path):\n",
    "    \"\"\"Zlicza liczbę wierszy w pliku.\"\"\"\n",
    "    row_count = 0\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile, delimiter='\\t')\n",
    "        for _ in reader:\n",
    "            row_count += 1\n",
    "    return row_count\n",
    "\n",
    "def compare_row_counts(file1, file2):\n",
    "    \"\"\"Porównuje liczbę wierszy w dwóch plikach i zwraca różnicę.\"\"\"\n",
    "    count1 = count_rows(file1)\n",
    "    count2 = count_rows(file2)\n",
    "    difference = abs(count1 - count2)\n",
    "    return count1, count2, difference\n",
    "\n",
    "# Pliki do porównania\n",
    "file1 = 'unziped_data.tsv'\n",
    "file2 = 'final_cleaned_data.tsv'\n",
    "\n",
    "# Porównanie liczby wierszy i obliczenie różnicy\n",
    "count1, count2, difference = compare_row_counts(file1, file2)\n",
    "\n",
    "print(f\"Liczba wierszy w pliku {file1}: {count1}\")\n",
    "print(f\"Liczba wierszy w pliku {file2}: {count2}\")\n",
    "print(f\"Ilość zredukowanych błędnych rekordów: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych do DataFrame \n",
    "##### Dane są już w docelowym wymiarze, więc dalsze operacje będę wykonywał przy użyciu Pandas.\n",
    "##### Danych jest bardzo dużo i takie operacje efektywniej o wiele byłoby wykonać np. na clustrze Sparka. Dalsze działania wykonam na pierwszych 100 000 wierszach, aby pokazać efekty pracy i wykonane zadania, ale można też przy posiadaniu dużej mocy obliczeniowej pominąć ten warunek lub zwiększyć ilość rekordów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie danych do DataFrame i dodanie nazw kolumn\n",
    "\n",
    "df = pd.read_csv('final_cleaned_data.tsv', sep='\\t', nrows=100000, header = None)\n",
    "\n",
    "df.columns = ['ID użytkownika', 'Adres URL', 'Unix timestamp']\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzam czy finalny wymiar DataFrame jest poprawny\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzam typ danych\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalne czyszczenie danych i przygotowanie ich do analizy\n",
    "##### Na potrzebę wykonania zadania, rozdzielam domeny od podstron i usuwam protokół HTTP oraz oczyszczam z błędów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiuje funkcję usuwającą protokół HTTP\n",
    "def split_url(url):\n",
    "    # Regex do wyodrębnienia głównej domeny od podstrony\n",
    "    match = re.match(r'https?://([^/?#]+)([^?#]*)', url)\n",
    "    if match:\n",
    "        domain = match.group(1)\n",
    "        path = match.group(2)\n",
    "        return domain, path\n",
    "    else:\n",
    "        return url, '' \n",
    "\n",
    "# Zastosowanie funkcji do kolumny 'Adres URL'\n",
    "df[['Domena', 'Podstrona']] = df['Adres URL'].apply(split_url).apply(pd.Series)\n",
    "\n",
    "# Usunięcie oryginalnej kolumny 'Adres URL'\n",
    "df = df.drop(columns=['Adres URL'])\n",
    "\n",
    "# Przearanżowanie kolumn\n",
    "df = df[['Unix timestamp', 'ID użytkownika', 'Domena', 'Podstrona']]\n",
    "\n",
    "# Usuwam protokół HTTP\n",
    "df['Domena'] = df['Domena'].str.replace(r'^https?://', '', regex=True)\n",
    "\n",
    "# Czyszczę błędne wartości w kolumnie 'Podstrona'\n",
    "df['Podstrona'] = df['Podstrona'].apply(lambda x: '' if x.strip() == '/' else x)\n",
    "\n",
    "# Sortowanie danych\n",
    "df = df.sort_values(by=['ID użytkownika', 'Unix timestamp', 'Domena', 'Podstrona'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja typu danych\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetlenie pierwszych 10 wierszy\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1. Liczba unikalnych użytkowników, którzy odwiedzili każdą z domen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task1_df = df.copy()\n",
    "# Grupowanie po domenie i liczenie unikalnych ID\n",
    "unique_users = task1_df.groupby('Domena')['ID użytkownika'].nunique().reset_index()\n",
    "\n",
    "# Zmiana nazw kolumn\n",
    "unique_users = unique_users.rename(columns={'ID użytkownika': 'Liczba unikalnych użytkowników'})\n",
    "\n",
    "\n",
    "# Zapisanie wyników do pliku TSV\n",
    "unique_users.to_csv('unique_users_per_domain.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2. Liczba podstron odwiedzanych przez typowego użytkownika na każdej domenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_df = df.copy()\n",
    "\n",
    "# Zamiana pustych wartości na 0\n",
    "task2_df['Podstrona'] = task2_df['Podstrona'].apply(lambda x: 1 if x.strip() != '' else 0)\n",
    "\n",
    "# Zliczanie liczby podstron\n",
    "user_page_count = task2_df.groupby(['Domena', 'ID użytkownika'])['Podstrona'].sum().reset_index()\n",
    "\n",
    "# średni liczba podstron na użytkownika dla każdej domeny\n",
    "average_pages_per_domain = user_page_count.groupby('Domena')['Podstrona'].mean().reset_index()\n",
    "\n",
    "# Zmiana nazwy kolumn\n",
    "average_pages_per_domain = average_pages_per_domain.rename(columns={'Podstrona': 'Średnia liczba podstron na użytkownika'})\n",
    "\n",
    "# Dodanie kolumny z zaokrągloną liczbą podstron\n",
    "average_pages_per_domain['Zaokrąglona liczba podstron'] = average_pages_per_domain['Średnia liczba podstron na użytkownika'].round()\n",
    "\n",
    "# Zapisanie wyników do pliku TSV\n",
    "average_pages_per_domain.to_csv('average_pages_per_domain.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3. Bounce Rate-procent użytkowników, którzy odwiedzają daną stronę i nie wykonują na niej żadnej akcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_df = df.copy()\n",
    "\n",
    "# Sortowanie danych\n",
    "task3_df = task3_df.sort_values(by=['ID użytkownika', 'Unix timestamp', 'Domena', 'Podstrona'])\n",
    "\n",
    "# Definiuje funkcję do identyfikacji użytkowników nie wykonujących żadnych akcji na podstronach\n",
    "def get_bounce_users(group):\n",
    "    unique_users = group['ID użytkownika'].unique()\n",
    "    bounce_users = set()\n",
    "    for user in unique_users:\n",
    "        user_data = group[group['ID użytkownika'] == user]\n",
    "        if user_data['Podstrona'].eq('').all():\n",
    "            bounce_users.add(user)\n",
    "    return bounce_users\n",
    "\n",
    "# Grupowuje po domenie\n",
    "grouped = task3_df.groupby('Domena')\n",
    "\n",
    "# Obliczam liczbę unikalnych użytkowników i użytkowników niewykonujących akcji\n",
    "bounce_data = []\n",
    "for domain, group in grouped:\n",
    "    bounce_users = get_bounce_users(group)\n",
    "    total_users = group['ID użytkownika'].nunique()\n",
    "    bounce_users_count = len(bounce_users)\n",
    "    bounce_data.append({\n",
    "        'Domena': domain,\n",
    "        'Liczba unikalnych użytkowników': total_users,\n",
    "        'Użytkownicy tylko odwiedzający stronę': bounce_users_count\n",
    "    })\n",
    "\n",
    "bounce_rate_df = pd.DataFrame(bounce_data)\n",
    "\n",
    "# Obliczam wskaźnik Bounce Rate\n",
    "bounce_rate_df['(%) Użytkowników nie wykonujących żadnej akcji na stronie'] = (\n",
    "    bounce_rate_df['Użytkownicy tylko odwiedzający stronę'] / \n",
    "    bounce_rate_df['Liczba unikalnych użytkowników']\n",
    ") * 100\n",
    "\n",
    "# Zaokrąglenie do 2 miejsc po przecinku\n",
    "bounce_rate_df['(%) Użytkowników nie wykonujących żadnej akcji na stronie'] = (\n",
    "    bounce_rate_df['(%) Użytkowników nie wykonujących żadnej akcji na stronie'].round(2)\n",
    ")\n",
    "\n",
    "# Dodawanie %\n",
    "bounce_rate_df['(%) Użytkowników nie wykonujących żadnej akcji na stronie'] = (\n",
    "    bounce_rate_df['(%) Użytkowników nie wykonujących żadnej akcji na stronie'].astype(str) + '%'\n",
    ")\n",
    "\n",
    "# Zapisanie wyniku do pliku TSV\n",
    "bounce_rate_df.to_csv('bounce_rate_per_domain2.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4. Średni czas spędzany na domenie przez użytkownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task4_df = df.copy()\n",
    "\n",
    "# Sortowanie danych\n",
    "task4_df = task4_df.sort_values(by=['ID użytkownika', 'Unix timestamp', 'Domena', 'Podstrona'])\n",
    "\n",
    "# Definiuje funkcję wykrywającą użytkowników, wykonujących akcję na stronie\n",
    "def get_bounce_users(group):\n",
    "    unique_users = group['ID użytkownika'].unique()\n",
    "    no_bounce_users = set()\n",
    "    \n",
    "    for user in unique_users:\n",
    "        user_data = group[group['ID użytkownika'] == user]\n",
    "        if not user_data['Podstrona'].eq('').all():\n",
    "            no_bounce_users.add(user)\n",
    "    \n",
    "    return no_bounce_users\n",
    "\n",
    "# Obliczam wskaźnik użytkowników wykonujących akcję na stronie\n",
    "def get_bounce_rate(df):\n",
    "    bounce_rate_data = []\n",
    "    for domain, group in df.groupby('Domena'):\n",
    "        no_bounce_users = get_bounce_users(group)\n",
    "        bounce_rate_data.append({\n",
    "            'Domena': domain,\n",
    "            'Liczba unikalnych użytkowników': group['ID użytkownika'].nunique(),\n",
    "            'Użytkownicy wykonujący akcję na stronie': len(no_bounce_users)\n",
    "        })\n",
    "    return pd.DataFrame(bounce_rate_data)\n",
    "\n",
    "no_bounce_rate_df = get_bounce_rate(task4_df)\n",
    "\n",
    "# Definiuje funkcję liczącą czas spędzony na stronie\n",
    "def calculate_time_spent(group):\n",
    "    user_times = []\n",
    "    for user_id, user_data in group.groupby('ID użytkownika'):\n",
    "        if len(user_data) > 0:\n",
    "            start_time = user_data['Unix timestamp'].min()\n",
    "            end_time = user_data['Unix timestamp'].max()\n",
    "            time_spent = end_time - start_time\n",
    "            user_times.append(time_spent)\n",
    "    return pd.Series({\n",
    "        'Czas spędzony (ms)': sum(user_times)\n",
    "    })\n",
    "\n",
    "# Obliczam czas spędzony na stronie\n",
    "def get_time_spent(df):\n",
    "    time_spent_data = []\n",
    "    for domain, group in df.groupby('Domena'):\n",
    "        time_spent_data.append({\n",
    "            'Domena': domain,\n",
    "            **calculate_time_spent(group)\n",
    "        })\n",
    "    return pd.DataFrame(time_spent_data)\n",
    "\n",
    "time_spent_df = get_time_spent(task4_df)\n",
    "\n",
    "# Łączę oba DataFrame w jeden\n",
    "final_df = pd.merge(no_bounce_rate_df, time_spent_df, on='Domena')\n",
    "\n",
    "# Obliczam średni czas spędzany na stronie (w milisekundach)\n",
    "final_df['Średni czas spędzany (ms)'] = final_df['Czas spędzony (ms)'] / final_df['Użytkownicy wykonujący akcję na stronie']\n",
    "\n",
    "# Zapisuje wynik do pliku TSV\n",
    "final_df.to_csv('average_time_spent_per_domain.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
